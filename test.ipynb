{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b558c5e-2921-49f3-8c7a-15adf2484956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode finished after 15.000000 time steps / mean 0.000000\n",
      "1 Episode finished after 43.000000 time steps / mean -1.860000\n",
      "2 Episode finished after 13.000000 time steps / mean -3.440000\n",
      "3 Episode finished after 16.000000 time steps / mean -5.320000\n",
      "4 Episode finished after 19.000000 time steps / mean -7.170000\n",
      "5 Episode finished after 29.000000 time steps / mean -8.990000\n",
      "6 Episode finished after 21.000000 time steps / mean -10.710000\n",
      "7 Episode finished after 16.000000 time steps / mean -12.510000\n",
      "8 Episode finished after 15.000000 time steps / mean -14.360000\n",
      "9 Episode finished after 15.000000 time steps / mean -16.220000\n",
      "10 Episode finished after 17.000000 time steps / mean -18.080000\n",
      "11 Episode finished after 41.000000 time steps / mean -19.920000\n",
      "12 Episode finished after 19.000000 time steps / mean -21.520000\n",
      "13 Episode finished after 30.000000 time steps / mean -23.340000\n",
      "14 Episode finished after 57.000000 time steps / mean -25.050000\n",
      "15 Episode finished after 51.000000 time steps / mean -26.490000\n",
      "16 Episode finished after 21.000000 time steps / mean -27.990000\n",
      "17 Episode finished after 29.000000 time steps / mean -29.790000\n",
      "18 Episode finished after 90.000000 time steps / mean -31.510000\n",
      "19 Episode finished after 12.000000 time steps / mean -32.620000\n",
      "20 Episode finished after 9.000000 time steps / mean -34.510000\n",
      "21 Episode finished after 52.000000 time steps / mean -36.430000\n",
      "22 Episode finished after 15.000000 time steps / mean -37.920000\n",
      "23 Episode finished after 43.000000 time steps / mean -39.780000\n",
      "24 Episode finished after 62.000000 time steps / mean -41.360000\n",
      "25 Episode finished after 38.000000 time steps / mean -42.750000\n",
      "26 Episode finished after 99.000000 time steps / mean -44.380000\n",
      "27 Episode finished after 10.000000 time steps / mean -45.400000\n",
      "28 Episode finished after 11.000000 time steps / mean -47.310000\n",
      "29 Episode finished after 81.000000 time steps / mean -49.210000\n",
      "30 Episode finished after 106.000000 time steps / mean -50.410000\n",
      "31 Episode finished after 128.000000 time steps / mean -51.360000\n",
      "32 Episode finished after 28.000000 time steps / mean -52.090000\n",
      "33 Episode finished after 26.000000 time steps / mean -53.820000\n",
      "34 Episode finished after 36.000000 time steps / mean -55.570000\n",
      "36 Episode finished after 120.000000 time steps / mean -57.220000\n",
      "37 Episode finished after 31.000000 time steps / mean -58.030000\n",
      "38 Episode finished after 57.000000 time steps / mean -59.730000\n",
      "39 Episode finished after 9.000000 time steps / mean -61.170000\n",
      "40 Episode finished after 28.000000 time steps / mean -63.090000\n",
      "41 Episode finished after 51.000000 time steps / mean -64.820000\n",
      "42 Episode finished after 11.000000 time steps / mean -66.320000\n",
      "43 Episode finished after 165.000000 time steps / mean -68.220000\n",
      "44 Episode finished after 83.000000 time steps / mean -68.580000\n",
      "45 Episode finished after 22.000000 time steps / mean -69.760000\n",
      "46 Episode finished after 140.000000 time steps / mean -71.550000\n",
      "47 Episode finished after 39.000000 time steps / mean -72.160000\n",
      "48 Episode finished after 31.000000 time steps / mean -73.780000\n",
      "49 Episode finished after 74.000000 time steps / mean -75.480000\n",
      "50 Episode finished after 166.000000 time steps / mean -76.750000\n",
      "51 Episode finished after 178.000000 time steps / mean -77.100000\n",
      "52 Episode finished after 41.000000 time steps / mean -77.330000\n",
      "53 Episode finished after 57.000000 time steps / mean -78.930000\n",
      "54 Episode finished after 87.000000 time steps / mean -80.370000\n",
      "55 Episode finished after 70.000000 time steps / mean -81.510000\n",
      "56 Episode finished after 67.000000 time steps / mean -82.820000\n",
      "57 Episode finished after 93.000000 time steps / mean -84.160000\n",
      "58 Episode finished after 63.000000 time steps / mean -85.240000\n",
      "59 Episode finished after 69.000000 time steps / mean -86.620000\n",
      "60 Episode finished after 89.000000 time steps / mean -87.940000\n",
      "61 Episode finished after 56.000000 time steps / mean -89.060000\n",
      "62 Episode finished after 81.000000 time steps / mean -90.510000\n",
      "63 Episode finished after 56.000000 time steps / mean -91.710000\n",
      "64 Episode finished after 119.000000 time steps / mean -93.160000\n",
      "65 Episode finished after 68.000000 time steps / mean -93.980000\n",
      "66 Episode finished after 106.000000 time steps / mean -95.310000\n",
      "69 Episode finished after 119.000000 time steps / mean -96.260000\n",
      "80 Episode finished after 172.000000 time steps / mean -97.080000\n",
      "112 Episode finished after 198.000000 time steps / mean -97.370000\n",
      "120 Episode finished after 49.000000 time steps / mean -95.390000\n",
      "122 Episode finished after 64.000000 time steps / mean -96.910000\n",
      "123 Episode finished after 124.000000 time steps / mean -98.280000\n",
      "124 Episode finished after 76.000000 time steps / mean -99.050000\n",
      "125 Episode finished after 114.000000 time steps / mean -100.300000\n",
      "126 Episode finished after 84.000000 time steps / mean -101.170000\n",
      "127 Episode finished after 124.000000 time steps / mean -102.340000\n",
      "128 Episode finished after 118.000000 time steps / mean -103.110000\n",
      "129 Episode finished after 175.000000 time steps / mean -103.940000\n",
      "130 Episode finished after 161.000000 time steps / mean -104.200000\n",
      "131 Episode finished after 135.000000 time steps / mean -104.600000\n",
      "139 Episode finished after 178.000000 time steps / mean -105.260000\n",
      "143 Episode finished after 165.000000 time steps / mean -105.490000\n",
      "192 Episode finished after 127.000000 time steps / mean -105.850000\n",
      "194 Episode finished after 176.000000 time steps / mean -106.590000\n",
      "196 Episode finished after 78.000000 time steps / mean -106.840000\n",
      "199 Episode finished after 54.000000 time steps / mean -108.070000\n",
      "255 Episode finished after 138.000000 time steps / mean -109.540000\n",
      "256 Episode finished after 130.000000 time steps / mean -110.170000\n",
      "258 Episode finished after 146.000000 time steps / mean -110.880000\n",
      "259 Episode finished after 181.000000 time steps / mean -111.430000\n",
      "261 Episode finished after 121.000000 time steps / mean -111.630000\n",
      "560 Episode finished after 177.000000 time steps / mean -112.430000\n",
      "561 Episode finished after 24.000000 time steps / mean -112.670000\n",
      "562 Episode finished after 198.000000 time steps / mean -114.440000\n",
      "566 Episode finished after 36.000000 time steps / mean -112.460000\n",
      "567 Episode finished after 161.000000 time steps / mean -114.110000\n",
      "568 Episode finished after 138.000000 time steps / mean -114.510000\n",
      "569 Episode finished after 39.000000 time steps / mean -115.140000\n",
      "571 Episode finished after 53.000000 time steps / mean -116.760000\n",
      "573 Episode finished after 122.000000 time steps / mean -118.240000\n",
      "574 Episode finished after 189.000000 time steps / mean -119.030000\n",
      "625 Episode finished after 184.000000 time steps / mean -117.290000\n",
      "629 Episode finished after 102.000000 time steps / mean -115.880000\n",
      "631 Episode finished after 167.000000 time steps / mean -114.990000\n",
      "637 Episode finished after 107.000000 time steps / mean -113.480000\n",
      "653 Episode finished after 193.000000 time steps / mean -112.600000\n",
      "677 Episode finished after 144.000000 time steps / mean -110.960000\n",
      "678 Episode finished after 9.000000 time steps / mean -109.730000\n",
      "680 Episode finished after 183.000000 time steps / mean -109.800000\n",
      "681 Episode finished after 175.000000 time steps / mean -108.120000\n",
      "682 Episode finished after 112.000000 time steps / mean -106.520000\n",
      "683 Episode finished after 167.000000 time steps / mean -105.570000\n",
      "685 Episode finished after 146.000000 time steps / mean -104.310000\n",
      "687 Episode finished after 115.000000 time steps / mean -103.040000\n",
      "690 Episode finished after 129.000000 time steps / mean -102.190000\n",
      "691 Episode finished after 119.000000 time steps / mean -101.470000\n",
      "694 Episode finished after 155.000000 time steps / mean -100.790000\n",
      "696 Episode finished after 116.000000 time steps / mean -99.450000\n",
      "697 Episode finished after 110.000000 time steps / mean -98.580000\n",
      "698 Episode finished after 89.000000 time steps / mean -98.380000\n",
      "699 Episode finished after 120.000000 time steps / mean -97.610000\n",
      "700 Episode finished after 10.000000 time steps / mean -96.500000\n",
      "701 Episode finished after 79.000000 time steps / mean -96.920000\n",
      "702 Episode finished after 88.000000 time steps / mean -96.280000\n",
      "703 Episode finished after 192.000000 time steps / mean -95.830000\n",
      "704 Episode finished after 167.000000 time steps / mean -94.530000\n",
      "705 Episode finished after 131.000000 time steps / mean -93.240000\n",
      "707 Episode finished after 110.000000 time steps / mean -92.920000\n",
      "708 Episode finished after 98.000000 time steps / mean -91.920000\n",
      "767 Episode finished after 122.000000 time steps / mean -91.050000\n",
      "783 Episode finished after 129.000000 time steps / mean -90.640000\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# [0]ライブラリのインポート\n",
    "import gym  #倒立振子(cartpole)の実行環境\n",
    "from gym import wrappers  #gymの画像保存\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# [1]Q関数を離散化して定義する関数　------------\n",
    "# 観測した状態を離散値にデジタル変換する\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "\n",
    "\n",
    "# 各値を離散値に変換\n",
    "def digitize_state(observation):\n",
    "    #print(observation)\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n",
    "\n",
    "\n",
    "# [2]行動a(t)を求める関数 -------------------------------------\n",
    "def get_action(next_state, episode):    # 徐々に最適行動のみをとる、ε-greedy法\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action\n",
    "\n",
    "\n",
    "# update Q table  -------------------------------------\n",
    "def update_Qtable_sarsa(q_table, state, action, reward, next_state, next_action):\n",
    "    gamma = 0.99\n",
    "    alpha = 0.5\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
    "            alpha * (reward + gamma * q_table[next_state, next_action])\n",
    "\n",
    "    return q_table\n",
    "\n",
    "\n",
    "# set up parameters --------------------------------------------------------\n",
    "env = gym.make('CartPole-v1')\n",
    "max_number_of_steps = 200  #1試行のstep数\n",
    "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n",
    "#num_episodes = 2000  #総試行回数\n",
    "num_episodes = 1000  #総試行回数\n",
    "goal_average_reward = 195  #この報酬を超えると学習終了（中心への制御なし）\n",
    "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\n",
    "num_dizitized = 6  #分割数\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\n",
    "final_x = np.zeros((num_episodes, 1))  #学習後、各試行のt=200でのｘの位置を格納\n",
    "islearned = 0  #flag to check if learning is done \n",
    "isrender = 0  #描画フラグ\n",
    "\n",
    "\n",
    "# [5] メインルーチン--------------------------------------------------\n",
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    # 環境の初期化\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation[0])\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps):  #1試行のループ\n",
    "        if islearned == 1:  #学習終了したらcartPoleを描画する\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "            print (observation[0])  #カートのx位置を出力\n",
    "\n",
    "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
    "        observation, reward, done, info, extra = env.step(action)\n",
    "\n",
    "        # 報酬を設定し与える\n",
    "        if done:\n",
    "            if t < 195:\n",
    "                reward = -200  #こけたら罰則\n",
    "            else:\n",
    "                reward = 1  #立ったまま終了時は罰則はなし\n",
    "        else:\n",
    "            reward = 1  #各ステップで立ってたら報酬追加\n",
    "\n",
    "        episode_reward += reward  #報酬を追加\n",
    "\n",
    "        # 離散状態s_{t+1}を求める\n",
    "        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\n",
    "\n",
    "        #　＊ここがQlearningと異なる＊\n",
    "        next_action = get_action(next_state, episode)    # 次の行動a_{t+1}を求める\n",
    "        q_table = update_Qtable_sarsa(q_table, state, action, reward, next_state, next_action)\n",
    "\n",
    "        # 次の行動と状態に更新\n",
    "        action = next_action    # a_{t+1}\n",
    "        state = next_state      # s_{t+1}\n",
    "\n",
    "        # 終了時の処理\n",
    "        if done:\n",
    "            print('%d Episode finished after %f time steps / mean %f' %\n",
    "                  (episode, t + 1, total_reward_vec.mean()))\n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:],\n",
    "                                          episode_reward))  #報酬を記録\n",
    "            if islearned == 1:  #学習終わってたら最終のx座標を格納\n",
    "                final_x[episode, 0] = observation[0]\n",
    "            break\n",
    "\n",
    "    if (total_reward_vec.mean() >=\n",
    "            goal_average_reward):  # 直近の100エピソードが規定報酬以上であれば成功\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        islearned = 1\n",
    "        #np.savetxt('learned_Q_table.csv',q_table, delimiter=\",\") #Qtableの保存する場合\n",
    "        if isrender == 0:\n",
    "            #env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "            isrender = 1\n",
    "    #10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
    "    #if episode>10:\n",
    "    #    if isrender == 0:\n",
    "    #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "    #        isrender = 1\n",
    "    #    islearned=1;\n",
    "\n",
    "if islearned:\n",
    "    np.savetxt('final_x.csv', final_x, delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ceca2-d7f3-4acd-8e83-67e1a55e4000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
