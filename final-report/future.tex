\section{Conclusion and Future Work}

Throughout the course of this project, we were able to implement a SARSA and a Q-Learning algorithm that successfully played the CartPole game. 
As hypothesized, exponential decay of $\epsilon$ yielded the largest average reward. We also compared our methods' performance with existing metrics
and found that it matched related works. 

In the future, we wished to be able to save our modesl to memory, and be able to recall them in a fresh environment without having to go through the training
steps again. Moreover, we also wanted to implement eligibility traces with backwards TD($\lambda$) to see how it compares.

Finally, aside from this CartPole environment we used, there exists the Pendulum environment within the same Gym package with similar properties. We think this
will provide an interesting challenge and comparison to solve.